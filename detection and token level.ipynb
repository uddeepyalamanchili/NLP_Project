{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f86bba-6de8-449a-a437-c80f283f8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Load a smaller subset of sentences and labels for initial testing\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=5):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a68907-d675-4141-ac3b-cf5196342e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3319468.67it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3472020.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡§Ç .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡•Ä ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .', '‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡§æ .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡§æ ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .'] [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Tokenizing Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "print(len(sentences), sentences, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize sentences in smaller batches to avoid memory overload\n",
    "batch_size = 500\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(sentences), batch_size), desc=\"Batch Tokenizing Sentences\"):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch_sentences,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,  # Reduced max length for faster processing\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids_list.append(tokenized_batch[\"input_ids\"])\n",
    "    attention_mask_list.append(tokenized_batch[\"attention_mask\"])\n",
    "\n",
    "# Concatenate tokenized tensors\n",
    "tokenized_inputs = {\n",
    "    \"input_ids\": torch.cat(input_ids_list, dim=0),\n",
    "    \"attention_mask\": torch.cat(attention_mask_list, dim=0)\n",
    "}\n",
    "del input_ids_list, attention_mask_list  # Free up memory after concatenation\n",
    "gc.collect()  # Explicit garbage collection\n",
    "\n",
    "# Convert labels to a tensor\n",
    "labels = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d514eb-11e4-4cfb-920e-5a3fb4854293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataset with Pre-tokenized Inputs\n",
    "class PreTokenizedSentenceDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_inputs.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PreTokenizedSentenceDataset(tokenized_inputs, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb59715c-7181-47e0-b86e-bf3b141a78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/3 00:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=0.1769561767578125, metrics={'train_runtime': 7.7626, 'train_samples_per_second': 3.092, 'train_steps_per_second': 0.386, 'total_flos': 264934797312.0, 'train_loss': 0.1769561767578125, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification and move it to the GPU if available\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Define Training Arguments with Improvements\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Define the Trainer with compute_metrics and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# Train the Model with a Progress Bar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e4e553-003b-4f9e-866b-15afcb290a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7060546875, 'eval_accuracy': 0.5, 'eval_runtime': 0.0083, 'eval_samples_per_second': 241.747, 'eval_steps_per_second': 120.873, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./error_detection_model/tokenizer_config.json',\n",
       " './error_detection_model/special_tokens_map.json',\n",
       " './error_detection_model/vocab.txt',\n",
       " './error_detection_model/added_tokens.json',\n",
       " './error_detection_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the Model\n",
    "model.save_pretrained(\"./error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./error_detection_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89d7000-ac2d-4e19-b180-3eff44b2d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is error-free.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "sentence = \"‡§â‡§∏‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ ‡§ï‡•Ä ‡§ó‡§π‡§∞‡§æ‡§à ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§®‡§ú‡§æ‡§®‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ú‡•à‡§∏‡§æ ‡§π‡•à\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get prediction\n",
    "prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "if prediction == 1:\n",
    "    print(\"Sentence contains errors.\")\n",
    "else:\n",
    "    print(\"Sentence is error-free.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6e0984-b6df-41bd-98a2-3327b5f3ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels2(src_path, tgt_path, limit=20000000):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8a09ff-8a7c-49cc-8178-60b6185f9fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3441489.78it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3369125.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels2(src_path, tgt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f23112-beda-4cb7-b8fc-0c8e8765e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['‡§Æ‡§ß‡•Å ‡§®‡•á ‡§è‡§ï ‡§Æ‡§π‡§≤ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§î‡§∞ ‡§â‡§∏ ‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•Ä ‡§®‡§æ‡§Æ ‡§Æ‡§ß‡•Å‡§™‡•Å‡§∞‡•Ä ( ‡§∏‡§Ç‡§≠‡§µ‡§§‡§É ‡§Æ‡§•‡•Å‡§∞‡§æ ‡§Ö‡§¨ ) ‡§∞‡§ñ‡§æ .',\n",
       "  '‡§ï‡§ø‡§∏‡•Ä ‡§è‡§ï ‡§â‡§≤‡•ç‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§à ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§ì‡§Ç ‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§Ö‡§ß‡•ç‡§Ø‡§Ø‡§® ‡§≠‡§æ‡§∑‡§æ ‡§≠‡•Ç‡§ó‡•ã‡§≤ ‡§Ø‡§æ ‡§¨‡•ã‡§≤‡•Ä ‡§≠‡•Ç‡§ó‡•ã‡§≤ ‡§ï‡•Ä ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§Ü‡§§‡§æ ‡§π‡•à .',\n",
       "  '\" ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§™‡•Ç‡§õ‡•á ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® \" ‡§≤‡•â‡§®‡•ç‡§ö ‡§ï‡§ø‡§Ø‡§æ , .',\n",
       "  '‡§Ø‡§π ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§ï‡§ø ‡§∏‡§π‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≤‡§ø‡§Ø‡•á ‡§®‡•á‡§§‡•É‡§§‡•ç‡§µ ‡§ï‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§§‡§æ ‡§π‡•à .',\n",
       "  '‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ï‡•á ‡§™‡•Ç‡§∞‡•ç‡§µ ‡§õ‡§æ‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§∞‡•ç‡§ï‡•á‡§∂‡§ø‡§Ø‡§® ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡§Ç .',\n",
       "  '‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§∏‡§®‡•Ä ‡§¶‡•á‡§ì‡§≤ ‡§ï‡•ã ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ú‡•ç‡§Ø‡•Å‡§∞‡•Ä ‡§Ö‡§µ‡§æ‡§∞‡•ç‡§° ‡§ï‡•á ‡§§‡•å‡§∞ ‡§™‡§∞ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§™‡•Å‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§ì‡§∞ ‡§∏‡•á ‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§ï‡§æ ‡§ñ‡§ø‡§§‡§æ‡§¨ ‡§®‡§µ‡§æ‡§ú‡•Ä ‡§ó‡§Ø‡§æ .',\n",
       "  '‡§Æ‡§π‡§æ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§Ø‡•Å‡§¶‡•ç‡§ß ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∂‡§ø‡§∞‡§æ‡§ú ‡§®‡•á ‡§™‡§æ‡§Ç‡§°‡§µ‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¶‡§ø‡§Ø‡§æ ‡§•‡§æ .',\n",
       "  '‡•ß‡•Ø‡•¶‡•ß ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§ü‡§¶‡•ç‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§®‡§ó‡§∞ ‡§ï‡§æ ‡§¶‡§∞‡•ç‡§ú‡§æ ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ , ‡§§‡§•‡§æ ‡§â‡§∏‡•Ä ‡§µ‡§∞‡•ç‡§∑ ‡§π‡•Å‡§à ‡§™‡•ç‡§∞‡§•‡§Æ ‡§ú‡§®‡§ó‡§£‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§®‡§ó‡§∞ ‡§ï‡§æ ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡•ß‡•¶‡•®‡•Ø ‡§•‡•Ä .'],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "numbers = random.sample(range(2607707, 2607857), k=20)\n",
    "newsentences = []\n",
    "newlabels = []\n",
    "for i in numbers:\n",
    "    if labels[i]!=0:\n",
    "        newsentences.append(sentences[i])\n",
    "        newlabels.append(labels[i])\n",
    "\n",
    "newsentences, newlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c00461-a98f-4bdc-86df-357660929a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence contains errors.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "for i in newsentences:\n",
    "    sentence =i\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    if prediction == 1:\n",
    "        print(\"Sentence contains errors.\")\n",
    "    else:\n",
    "        print(\"Sentence is error-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19574aae-0686-43f3-8419-0af5d2636f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels3(src_file, tgt_file, limit = 10):\n",
    "    with open(src_file,'r', encoding='utf-8') as src, open(tgt_file,'r', encoding = 'utf-8') as tgt:\n",
    "        IncorrectSentences = [line.strip() for line in src.readlines()]\n",
    "        CorrectSentences = [line.strip() for line in tgt.readlines()]\n",
    "        print(IncorrectSentences[:limit])\n",
    "        print(CorrectSentences[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca6c291-f158-4add-be68-260cc362409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡§Ç .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡•Ä ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .', '‡§Ø‡§π‡§æ‡§Ç ‡§™‡§π‡§æ‡§°‡§º‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§ß‡•ç‡§Ø ‡§´‡•à‡§≤‡•Ä ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡§ø‡§∏‡•Ä ‡§ó‡•á‡§∏‡•ç‡§ü ‡§π‡§æ‡§â‡§∏ ‡§Æ‡•á‡§Ç ‡§∞‡•Å‡§ï ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§Æ‡§ú‡§æ ‡§â‡§†‡§æ ‡§∏‡§ï‡§§‡•Ä‡§Ç ‡§π‡•à‡§Ç .', '‡§∞‡§æ‡§Æ‡§®‡§æ‡§• ‡§â‡§∏‡§ï‡•Ä ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞‡•Ä ‡§¶‡•á‡§ñ ‡§ï‡§∞ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§™‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§â‡§∏‡•á ‡§ï‡•ç‡§≤‡§∞‡•ç‡§ï ‡§ï‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¶‡•á ‡§¶‡•á‡§§‡•Ä‡§Ç ‡§π‡•à .', '‡§ï‡§à ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§è‡§ï ‡§∏‡§§‡•ç‡§∞‡§π ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§ï‡•á‡§¨‡§ø‡§® ‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§≠‡•Ç‡§ñ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§™‡§æ‡§®‡•Ä ‡§™‡•Ä ‡§≤‡•á‡§®‡•Ä ‡§ï‡•Ä ‡§µ‡§ú‡§π ‡§∏‡•á ‡§¨‡•á‡§π‡•ã‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à .', '‡§á‡§®‡§ï‡•Ä ‡§∞‡§π‡§® ‡§∏‡§π‡§® ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§•‡§æ .', '‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ , ‡§Æ‡§à 2007 ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§ï‡§æ ‡§Æ‡§ï‡•ç‡§ï‡§æ ‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§Æ‡•á‡§Ç 11 ‡§ï‡•Ä ‡§Æ‡•å‡§§ .']\n",
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡§æ .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡§æ ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .', '‡§Ø‡§π‡§æ‡§Ç ‡§™‡§π‡§æ‡§°‡§º‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§ß‡•ç‡§Ø ‡§´‡•à‡§≤‡•Ä ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡§ø‡§∏‡•Ä ‡§ó‡•á‡§∏‡•ç‡§ü ‡§π‡§æ‡§â‡§∏ ‡§Æ‡•á‡§Ç ‡§∞‡•Å‡§ï ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§Æ‡§ú‡§æ ‡§â‡§†‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç .', '‡§∞‡§æ‡§Æ‡§®‡§æ‡§• ‡§â‡§∏‡§ï‡•Ä ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞‡•Ä ‡§¶‡•á‡§ñ ‡§ï‡§∞ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§™‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§â‡§∏‡•á ‡§ï‡•ç‡§≤‡§∞‡•ç‡§ï ‡§ï‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¶‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à .', '‡§ï‡§à ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§è‡§ï ‡§∏‡§§‡•ç‡§∞‡§π ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§ï‡•á‡§¨‡§ø‡§® ‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§≠‡•Ç‡§ñ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§™‡§æ‡§®‡•Ä ‡§™‡•Ä ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§µ‡§ú‡§π ‡§∏‡•á ‡§¨‡•á‡§π‡•ã‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à .', '‡§á‡§®‡§ï‡§æ ‡§∞‡§π‡§® ‡§∏‡§π‡§® ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§•‡§æ .', '‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ , ‡§Æ‡§à 2007 ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§ï‡•Ä ‡§Æ‡§ï‡•ç‡§ï‡§æ ‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§Æ‡•á‡§Ç 11 ‡§ï‡•Ä ‡§Æ‡•å‡§§ .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1f4b0d-74a1-4654-9813-96008550bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3396297.64it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3524000.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡§æ .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .']\n",
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 9157.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels3(src_path, tgt_path, limit=2):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "    print(correct_sentences)\n",
    "    print(incorrect_sentences)\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87e2e2d-a69a-4754-9aa6-d89b6d9b7477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .',\n",
       "  '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .'],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254148e3-c4d2-442e-a336-877747037695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3390690.16it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3512584.87it/s]\n",
      "Generating Labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [01:20<00:00, 32475.84it/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3859' max='782328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3859/782328 07:39 < 25:44:57, 8.40 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    152\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    153\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    563\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:184\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 184\u001b[0m         {\n\u001b[1;32m    185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:185\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    184\u001b[0m         {\n\u001b[0;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=3000000):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len=64):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence and align labels with tokens\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Get input ids and attention mask\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Align the labels with tokens\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Map word_ids to tokens\n",
    "\n",
    "        labels_aligned = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels_aligned.append(-100)  # Ignore these tokens (e.g., padding tokens)\n",
    "            else:\n",
    "                labels_aligned.append(label[word_id] if word_id < len(label) else 0)\n",
    "        \n",
    "        labels_aligned = torch.tensor(labels_aligned)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_aligned\n",
    "        }\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TokenClassificationDataset(sentences, labels, tokenizer)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Load pre-trained BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2  # Binary classification: correct or incorrect\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./token_classification_results\",\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",          # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = torch.argmax(torch.tensor(pred.predictions), axis=-1).flatten()\n",
    "\n",
    "    # Filter out -100 labels (ignored labels for padding, etc.)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./token_error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./token_error_detection_model\")\n",
    "\n",
    "# Make predictions for new sentence\n",
    "sentence = \"‡§Ø‡§π ‡§è‡§ï ‡§ó‡§≤‡§§ ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to CUDA if available\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "# Map predictions back to tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "for token, prediction in zip(tokens, predictions[0][:len(tokens)]):\n",
    "    status = \"Incorrect\" if prediction == 1 else \"Correct\"\n",
    "    print(f\"Token: {token}, Status: {status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
