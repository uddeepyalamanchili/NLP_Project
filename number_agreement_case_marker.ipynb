{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dffae5-eecc-4dd8-b650-e069ad66c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza\n",
    "\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Initialize Stanza pipeline with GPU\n",
    "stanza.download(\"hi\")\n",
    "nlp = stanza.Pipeline(\"hi\", batch_size=1024, processors=\"tokenize,pos\", use_gpu=True)\n",
    "\n",
    "def generate_number_agreement_errors(doc):\n",
    "    \"\"\"\n",
    "    Generate number agreement errors for a processed Stanza document.\n",
    "    Args:\n",
    "        doc: Stanza processed sentence (doc.sentences[0]).\n",
    "    Returns:\n",
    "        str: Modified sentence with errors.\n",
    "    \"\"\"\n",
    "    modified_sentence = []\n",
    "\n",
    "    for word in doc.words:\n",
    "        if word.upos == \"NOUN\":\n",
    "            # Singular to plural or plural to singular transformation\n",
    "            if word.text.endswith(\"ा\"):  # Singular noun\n",
    "                modified_sentence.append(word.text[:-1] + \"े\")  # Replace ending\n",
    "            elif word.text.endswith(\"े\"):  # Plural noun\n",
    "                modified_sentence.append(word.text[:-1] + \"ा\")\n",
    "            else:\n",
    "                modified_sentence.append(word.text)\n",
    "        elif word.upos == \"ADJ\":\n",
    "            # Adjective Agreement: Match the transformed noun\n",
    "            if word.text.endswith(\"ा\"):\n",
    "                modified_sentence.append(word.text[:-1] + \"े\")\n",
    "            elif word.text.endswith(\"े\"):\n",
    "                modified_sentence.append(word.text[:-1] + \"ा\")\n",
    "            else:\n",
    "                modified_sentence.append(word.text)\n",
    "        elif word.upos == \"AUX\":\n",
    "            # Singular to plural or plural to singular auxiliary verbs\n",
    "            if word.text == \"हैं\":  # Plural auxiliary\n",
    "                modified_sentence.append(\"है\")  # Plural to singular\n",
    "            elif word.text == \"है\":  # Singular auxiliary\n",
    "                modified_sentence.append(\"हैं\")  # Singular to plural\n",
    "            else:\n",
    "                modified_sentence.append(word.text)\n",
    "        elif word.upos == \"PRON\":\n",
    "            # Pronoun Agreement: Singular to plural or vice versa\n",
    "            if word.text == \"उसका\":  # Singular pronoun\n",
    "                modified_sentence.append(\"उनका\")  # Singular to plural\n",
    "            elif word.text == \"उनका\":  # Plural pronoun\n",
    "                modified_sentence.append(\"उसका\")  # Plural to singular\n",
    "            elif word.text == \"अपना\":  # Reflexive pronoun\n",
    "                modified_sentence.append(\"अपने\")\n",
    "            elif word.text == \"अपने\":\n",
    "                modified_sentence.append(\"अपना\")\n",
    "            else:\n",
    "                modified_sentence.append(word.text)\n",
    "        else:\n",
    "            # Keep other tokens unchanged\n",
    "            modified_sentence.append(word.text)\n",
    "\n",
    "    return \" \".join(modified_sentence)\n",
    "\n",
    "\n",
    "def generate_case_marker_errors(doc):\n",
    "    \"\"\"\n",
    "    Generate case-marker errors for a processed Stanza document.\n",
    "    Args:\n",
    "        doc: Stanza processed sentence (doc.sentences[0]).\n",
    "    Returns:\n",
    "        str: Modified sentence with case-marker errors.\n",
    "    \"\"\"\n",
    "    modified_sentence = []\n",
    "\n",
    "    for word in doc.words:\n",
    "        # Identify nouns and modify associated case markers\n",
    "        if word.upos == \"ADP\":  # ADP (Adposition) includes case markers\n",
    "            if word.text == \"ने\":\n",
    "                modified_sentence.append(\"को\")  # Replace 'ने' with 'को'\n",
    "            elif word.text == \"को\":\n",
    "                modified_sentence.append(\"से\")  # Replace 'को' with 'से'\n",
    "            elif word.text == \"से\":\n",
    "                modified_sentence.append(\"का\")  # Replace 'से' with 'का'\n",
    "            elif word.text in [\"का\", \"की\", \"के\"]:\n",
    "                modified_sentence.append(\"ने\")  # Replace possessives with 'ने'\n",
    "            else:\n",
    "                modified_sentence.append(word.text)  # Keep other markers unchanged\n",
    "        else:\n",
    "            modified_sentence.append(word.text)\n",
    "\n",
    "    return \" \".join(modified_sentence)\n",
    "\n",
    "\n",
    "def process_batch(documents, error_type='number_agreement'):\n",
    "    \"\"\"\n",
    "    Process a batch of Stanza documents to generate errors.\n",
    "    Args:\n",
    "        documents: List of Stanza processed sentences.\n",
    "        error_type: Type of error to generate ('number_agreement' or 'case_marker').\n",
    "    Returns:\n",
    "        List of modified sentences with errors.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        if error_type == 'number_agreement':\n",
    "            results = list(executor.map(generate_number_agreement_errors, documents))\n",
    "        elif error_type == 'case_marker':\n",
    "            results = list(executor.map(generate_case_marker_errors, documents))\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_src_file_with_pooling(input_file, output_file, batch_size=2048, error_type='number_agreement'):\n",
    "    \"\"\"\n",
    "    Process a .src file to generate errors and save to a new file using pooling.\n",
    "    Args:\n",
    "        input_file (str): Path to the input .src file.\n",
    "        output_file (str): Path to the output .src file.\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "        error_type (str): Type of error to generate ('number_agreement' or 'case_marker').\n",
    "    \"\"\"\n",
    "    # Read all sentences from the input .src file\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "\n",
    "    # Initialize output\n",
    "    modified_sentences = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Processing Batches\"):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "\n",
    "        # Use Stanza to process the batch\n",
    "        doc = nlp(\"\\n\".join(batch))\n",
    "\n",
    "        # Collect sentences from the processed doc\n",
    "        documents = doc.sentences\n",
    "\n",
    "        # Generate errors using pooling\n",
    "        batch_modified_sentences = process_batch(documents, error_type=error_type)\n",
    "\n",
    "        # Write to output in chunks to avoid memory overflow\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as out_f:\n",
    "            out_f.writelines([line + \"\\n\" for line in batch_modified_sentences])\n",
    "\n",
    "    print(f\"Processed {len(sentences)} sentences. Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077076b9-e45b-4771-a2cc-516bee60e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"wikiExtractsData/data/train_merge.src\"\n",
    "output_file = \"train_data_with_case_errors.src\"\n",
    "\n",
    "# process_src_file_with_pooling(input_file, output_file, batch_size=64, error_type='number_agreement')\n",
    "\n",
    "output_file2 = \"train_data_case_marker_errors.src\"\n",
    "\n",
    "process_src_file_with_pooling(input_file, output_file2, batch_size=64, error_type='case_marker')\n",
    "\n",
    "\n",
    "# process_src_file_with_case_marker_errors(input_file, output_file, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0ff66-b1fa-4221-a4f2-6ba6dfe44171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify file paths\n",
    "input_file = \"wikiExtractsData/data/train_merge.src\"  # Path to your input .src file\n",
    "output_file = \"train_data_with_errors.src\"  # Path to save the modified .src file\n",
    "\n",
    "# Process the file\n",
    "process_src_file_optimized(input_file, output_file, batch_size=1024)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
