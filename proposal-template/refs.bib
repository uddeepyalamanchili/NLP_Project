@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}


@inproceedings{s_vyakranly_2023,
	title = {Vyakranly : {Hindi} {Grammar} \& {Spelling} {Errors} {Detection} and {Correction} {System}},
	shorttitle = {Vyakranly},
	url = {https://ieeexplore.ieee.org/document/10146610/?arnumber=10146610},
	doi = {10.1109/ICNTE56631.2023.10146610},
	abstract = {The growing demand for automation tools for Hindi over the past few years has led NLP experts to start working towards tasks that facilitate research and development for Hindi Language Processing. Researchers have been increasingly putting efforts into building models to perform essential NLP tasks like spell correction, grammar correction, summarizing, and so on. Compared to the plethora of tools and data available for English, Hindi is a relatively new area in which not much work has been done so far. Therefore, researchers need to build frameworks and technologies that support the Hindi language to perform such complex tasks effectively.However, with limited data and tools available for Hindi, performing Grammatical Error Correction (GEC) for Hindi may come across as a challenge. Therefore, we propose Vyakranly (व्याक्रणली), a Hindi Translation and Grammatical Error Detection Toolkit for the Indic language Hindi. Objectives of Vyakranly (व्याक्रणली) are Hindi Text Spelling Error Detection and correction, Hindi Sentence Grammar Error Detection and correction, English to Hindi and Hindi to English text translation. Highlights of our work are Hindi spelling detection as well as correction and grammar error detection. It is difficult to find such past work for the Hindi language due to their relative lack of digitized content and complex morphology, compared to English.},
	urldate = {2024-10-22},
	booktitle = {2023 5th {Biennial} {International} {Conference} on {Nascent} {Technologies} in {Engineering} ({ICNTE})},
	author = {S., Rachel and S., Vasudha and T., Shriya and K., Rhutuja and Gadhikar, Lakshmi},
	month = jan,
	year = {2023},
	keywords = {Automation, Buildings, Error Annotation, Error correction, Grammar, Grammarly, Hindi Grammatical Error Correction, Hindi to English Translation, Morphology, Natural Language Processing, Rule-Based Approaches, Sociology, Training},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\uddee\\Zotero\\storage\\IWI8TWNQ\\S. et al. - 2023 - Vyakranly  Hindi Grammar & Spelling Errors Detection and Correction System.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\uddee\\Zotero\\storage\\2JVBTHFP\\10146610.html:text/html},
}


@article{mittal_detection_2019,
	title = {Detection and {Correction} of {Grammatical} {Errors} in {Hindi} {Language} {Using} {Hybrid} {Approach}},
	volume = {7},
	url = {https://www.ijcseonline.org/pdf_paper_view.php?paper_id=4258&70-IJCSE-06935.pdf},
	number = {5},
	urldate = {2024-10-22},
	author = {Mittal, M. and Sharma, S. K. and Sethi, A.},
	month = may,
	year = {2019},
	pages = {421--426},
}


@misc{shahgir2023,
	title = {Bangla {Grammatical} {Error} {Detection} {Using} {T5} {Transformer} {Model}},
	url = {http://arxiv.org/abs/2303.10612},
	abstract = {This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonstrating the potential of T5 models for detecting grammatical errors in a wide range of languages.},
	urldate = {2024-10-25},
	publisher = {arXiv},
	author = {Shahgir, H. A. Z. Sameen and Sayeed, Khondker Salman},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10612},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\uddee\\Zotero\\storage\\CHCJJRA6\\Shahgir and Sayeed - 2023 - Bangla Grammatical Error Detection Using T5 Transformer Model.pdf:application/pdf;Snapshot:C\:\\Users\\uddee\\Zotero\\storage\\VJZZXUQZ\\2303.html:text/html},
}


@misc{raffel2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2024-10-25},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\uddee\\Zotero\\storage\\NBUL59G6\\Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:application/pdf;Snapshot:C\:\\Users\\uddee\\Zotero\\storage\\8XF7TSWG\\1910.html:text/html},
}
d

@inproceedings{singh2016frequency,
  title={Frequency based spell checking and rule based grammar checking},
  author={Singh, Shashi Pal and Kumar, Ajai and Singh, Lenali and Bhargava, Mahesh and Goyal, Kritika and Sharma, Bhanu},
  booktitle={2016 international conference on electrical, electronics, and optimization techniques (iceeot)},
  pages={4435--4439},
  year={2016},
  organization={IEEE}
}

@article{shahgir2023bangla,
  title={Bangla grammatical error detection using t5 transformer model},
  author={Shahgir, HAZ and Sayeed, Khondker Salman},
  journal={arXiv preprint arXiv:2303.10612},
  year={2023}
}