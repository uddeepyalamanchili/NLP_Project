{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03570a39-939c-4dd8-98ef-91ac15427b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f86bba-6de8-449a-a437-c80f283f8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Load a smaller subset of sentences and labels for initial testing\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=200000):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6a68907-d675-4141-ac3b-cf5196342e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3563704.57it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3563542.02it/s]\n",
      "Batch Tokenizing Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:10<00:00, 79.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "# print(len(sentences), sentences, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize sentences in smaller batches to avoid memory overload\n",
    "batch_size = 500\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(sentences), batch_size), desc=\"Batch Tokenizing Sentences\"):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch_sentences,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,  # Reduced max length for faster processing\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids_list.append(tokenized_batch[\"input_ids\"])\n",
    "    attention_mask_list.append(tokenized_batch[\"attention_mask\"])\n",
    "\n",
    "# Concatenate tokenized tensors\n",
    "tokenized_inputs = {\n",
    "    \"input_ids\": torch.cat(input_ids_list, dim=0),\n",
    "    \"attention_mask\": torch.cat(attention_mask_list, dim=0)\n",
    "}\n",
    "del input_ids_list, attention_mask_list  # Free up memory after concatenation\n",
    "gc.collect()  # Explicit garbage collection\n",
    "\n",
    "# Convert labels to a tensor\n",
    "labels = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d514eb-11e4-4cfb-920e-5a3fb4854293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataset with Pre-tokenized Inputs\n",
    "class PreTokenizedSentenceDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_inputs.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PreTokenizedSentenceDataset(tokenized_inputs, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb59715c-7181-47e0-b86e-bf3b141a78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  105/30000 00:08 < 40:33, 12.28 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     39\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the Model with a Progress Bar\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2534\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2534\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2538\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/optimizer.py:158\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 338\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:285\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 285\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/optimizer.py:203\u001b[0m, in \u001b[0;36mpatch_optimizer_step.<locals>.patched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched_step\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    202\u001b[0m     accelerated_optimizer\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/optim/adamw.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    260\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    266\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification and move it to the GPU if available\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Define Training Arguments with Improvements\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Define the Trainer with compute_metrics and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# Train the Model with a Progress Bar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4e553-003b-4f9e-866b-15afcb290a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the Model\n",
    "model.save_pretrained(\"./error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./error_detection_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89d7000-ac2d-4e19-b180-3eff44b2d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is error-free.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "sentence = \"‡§â‡§∏‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ ‡§ï‡•Ä ‡§ó‡§π‡§∞‡§æ‡§à ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§®‡§ú‡§æ‡§®‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ú‡•à‡§∏‡§æ ‡§π‡•à\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get prediction\n",
    "prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "if prediction == 1:\n",
    "    print(\"Sentence contains errors.\")\n",
    "else:\n",
    "    print(\"Sentence is error-free.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed6e0984-b6df-41bd-98a2-3327b5f3ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels2(src_path, tgt_path, limit=20000000):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8a09ff-8a7c-49cc-8178-60b6185f9fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3579380.97it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3557019.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels2(src_path, tgt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "69f23112-beda-4cb7-b8fc-0c8e8765e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä ‡§¶‡•ç‡§µ‡•Ä‡§™ ‡§Æ‡§ø‡§Ø‡§æ‡§Æ‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§≠‡•Ç‡§Æ‡§ø ‡§™‡§∞ ‡§Ö‡§µ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡§Ç .',\n",
       "  '‡§á‡§∏‡§≤‡§ø‡§è , ‡§ï‡•ã‡§à ‡§Ö‡§®‡•ç‡§Ø ‡§∏‡§π‡§ú ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§π‡•ã‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è ‡§ú‡•ã ‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§≠‡§æ‡§∑‡§æ ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á .',\n",
       "  '‡•ß‡•Ø‡•¶‡•ß ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§ü‡§¶‡•ç‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§®‡§ó‡§∞ ‡§ï‡§æ ‡§¶‡§∞‡•ç‡§ú‡§æ ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ , ‡§§‡§•‡§æ ‡§â‡§∏‡•Ä ‡§µ‡§∞‡•ç‡§∑ ‡§π‡•Å‡§à ‡§™‡•ç‡§∞‡§•‡§Æ ‡§ú‡§®‡§ó‡§£‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§®‡§ó‡§∞ ‡§ï‡§æ ‡§ú‡§®‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡•ß‡•¶‡•®‡•Ø ‡§•‡•Ä .',\n",
       "  '‡§®‡§µ‡•Ä‡§®‡§§‡§Æ ‡§Æ‡§π‡§≤ ‡§™‡§∞ ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£‡§ï‡§æ‡§∞‡•ç‡§Ø , ‡•ß‡•Æ‡•´‡•¨ ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•Å‡§Ü ‡§§‡§•‡§æ ‡§ï‡•Å‡§õ ‡§∏‡§Æ‡§Ø ‡§™‡§∂‡•ç‡§ö‡§æ‡§§‡•ç ‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§Æ‡§π‡§≤ ‡§ï‡•ã ‡§ß‡•ç‡§µ‡§∏‡•ç‡§§ ‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ .',\n",
       "  '‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§Æ‡•Ç‡§π ‡§ï‡•ã ‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§æ‡§ú ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç .',\n",
       "  '‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§∏‡§®‡•Ä ‡§¶‡•á‡§ì‡§≤ ‡§ï‡•ã ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ú‡•ç‡§Ø‡•Å‡§∞‡•Ä ‡§Ö‡§µ‡§æ‡§∞‡•ç‡§° ‡§ï‡•á ‡§§‡•å‡§∞ ‡§™‡§∞ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§´‡§ø‡§≤‡•ç‡§Æ ‡§™‡•Å‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§ì‡§∞ ‡§∏‡•á ‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§ï‡§æ ‡§ñ‡§ø‡§§‡§æ‡§¨ ‡§®‡§µ‡§æ‡§ú‡•Ä ‡§ó‡§Ø‡§æ .'],\n",
       " [1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "numbers = random.sample(range(2607707, 2607857), k=20)\n",
    "newsentences = []\n",
    "newlabels = []\n",
    "for i in numbers:\n",
    "    if labels[i]!=0:\n",
    "        newsentences.append(sentences[i])\n",
    "        newlabels.append(labels[i])\n",
    "\n",
    "newsentences, newlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81c00461-a98f-4bdc-86df-357660929a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence contains errors.\n",
      "Sentence is error-free.\n",
      "Sentence contains errors.\n",
      "Sentence contains errors.\n",
      "Sentence is error-free.\n",
      "Sentence contains errors.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "for i in newsentences:\n",
    "    sentence =i\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    if prediction == 1:\n",
    "        print(\"Sentence contains errors.\")\n",
    "    else:\n",
    "        print(\"Sentence is error-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19574aae-0686-43f3-8419-0af5d2636f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels3(src_file, tgt_file, limit = 10):\n",
    "    with open(src_file,'r', encoding='utf-8') as src, open(tgt_file,'r', encoding = 'utf-8') as tgt:\n",
    "        IncorrectSentences = [line.strip() for line in src.readlines()]\n",
    "        CorrectSentences = [line.strip() for line in tgt.readlines()]\n",
    "        print(IncorrectSentences[:limit])\n",
    "        print(CorrectSentences[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ca6c291-f158-4add-be68-260cc362409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à‡§Ç .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡•Ä ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡§æ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .', '‡§Ø‡§π‡§æ‡§Ç ‡§™‡§π‡§æ‡§°‡§º‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§ß‡•ç‡§Ø ‡§´‡•à‡§≤‡•Ä ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡§ø‡§∏‡•Ä ‡§ó‡•á‡§∏‡•ç‡§ü ‡§π‡§æ‡§â‡§∏ ‡§Æ‡•á‡§Ç ‡§∞‡•Å‡§ï ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§Æ‡§ú‡§æ ‡§â‡§†‡§æ ‡§∏‡§ï‡§§‡•Ä‡§Ç ‡§π‡•à‡§Ç .', '‡§∞‡§æ‡§Æ‡§®‡§æ‡§• ‡§â‡§∏‡§ï‡•Ä ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞‡•Ä ‡§¶‡•á‡§ñ ‡§ï‡§∞ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§™‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§â‡§∏‡•á ‡§ï‡•ç‡§≤‡§∞‡•ç‡§ï ‡§ï‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¶‡•á ‡§¶‡•á‡§§‡•Ä‡§Ç ‡§π‡•à .', '‡§ï‡§à ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§è‡§ï ‡§∏‡§§‡•ç‡§∞‡§π ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§ï‡•á‡§¨‡§ø‡§® ‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§≠‡•Ç‡§ñ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§™‡§æ‡§®‡•Ä ‡§™‡•Ä ‡§≤‡•á‡§®‡•Ä ‡§ï‡•Ä ‡§µ‡§ú‡§π ‡§∏‡•á ‡§¨‡•á‡§π‡•ã‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à .', '‡§á‡§®‡§ï‡•Ä ‡§∞‡§π‡§® ‡§∏‡§π‡§® ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§•‡§æ .', '‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ , ‡§Æ‡§à 2007 ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§ï‡§æ ‡§Æ‡§ï‡•ç‡§ï‡§æ ‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§Æ‡•á‡§Ç 11 ‡§ï‡•Ä ‡§Æ‡•å‡§§ .']\n",
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡§æ .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .', '‡§°‡•Ä‡§è‡§®‡§è ‡§ï‡•ç‡§∑‡§§‡§ø ‡§î‡§∞ ‡§â‡§§‡•ç‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§π‡•à .', '‡§Ø‡§π ‡§ñ‡§æ‡§®‡§æ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§ï‡§æ‡§Æ ‡§Ü‡§§‡§æ ‡§π‡•à .', '‡§´‡§º‡§ø‡§≤‡•ç‡§Æ ‡§ï‡•á ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ú‡§º‡•Ä ‡§Æ‡•ç‡§Ø‡•Ç‡§ú‡§ø‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•É‡§π‡•Ä‡§§ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§•‡•á , ‡§î‡§∞ ‡§è‡§≤‡•ç‡§¨‡§Æ ‡§ï‡•ã ‡•ß‡•ß ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•®‡•¶‡•ß‡•≠ ‡§ï‡•ã ‡§∞‡§ø‡§≤‡•Ä‡§ú‡§º ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ .', '‡§Ø‡§π‡§æ‡§Ç ‡§™‡§π‡§æ‡§°‡§º‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§ß‡•ç‡§Ø ‡§´‡•à‡§≤‡•Ä ‡§ù‡•Ä‡§≤ ‡§ï‡•á ‡§Ü‡§∏‡§™‡§æ‡§∏ ‡§ï‡§ø‡§∏‡•Ä ‡§ó‡•á‡§∏‡•ç‡§ü ‡§π‡§æ‡§â‡§∏ ‡§Æ‡•á‡§Ç ‡§∞‡•Å‡§ï ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡§æ ‡§Æ‡§ú‡§æ ‡§â‡§†‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç .', '‡§∞‡§æ‡§Æ‡§®‡§æ‡§• ‡§â‡§∏‡§ï‡•Ä ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞‡•Ä ‡§¶‡•á‡§ñ ‡§ï‡§∞ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§™‡§®‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§Æ‡•á‡§Ç ‡§â‡§∏‡•á ‡§ï‡•ç‡§≤‡§∞‡•ç‡§ï ‡§ï‡•Ä ‡§®‡•å‡§ï‡§∞‡•Ä ‡§¶‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à .', '‡§ï‡§à ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§è‡§ï ‡§∏‡§§‡•ç‡§∞‡§π ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§ï‡•á‡§¨‡§ø‡§® ‡§ï‡§∞‡•ç‡§Æ‡§ö‡§æ‡§∞‡•Ä ‡§≠‡•Ç‡§ñ ‡§î‡§∞ ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞‡•Ä ‡§™‡§æ‡§®‡•Ä ‡§™‡•Ä ‡§≤‡•á‡§®‡•á ‡§ï‡•Ä ‡§µ‡§ú‡§π ‡§∏‡•á ‡§¨‡•á‡§π‡•ã‡§∂ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à .', '‡§á‡§®‡§ï‡§æ ‡§∞‡§π‡§® ‡§∏‡§π‡§® ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∏‡§æ‡§ß‡§æ‡§∞‡§£ ‡§•‡§æ .', '‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ , ‡§Æ‡§à 2007 ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§ï‡•Ä ‡§Æ‡§ï‡•ç‡§ï‡§æ ‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∏‡•ç‡§´‡•ã‡§ü ‡§Æ‡•á‡§Ç 11 ‡§ï‡•Ä ‡§Æ‡•å‡§§ .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a1f4b0d-74a1-4654-9813-96008550bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3520131.88it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3567576.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡§æ .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .']\n",
      "['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .', '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 11667.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels3(src_path, tgt_path, limit=2):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "    print(correct_sentences)\n",
    "    print(incorrect_sentences)\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e87e2e2d-a69a-4754-9aa6-d89b6d9b7477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['‡§§‡§¨ ‡§∞‡§æ‡§ú‡§æ ‡§ï‡•ã ‡§Ü‡§≠‡§æ‡§∏ ‡§π‡•Å‡§Ü ‡§ï‡§ø ‡§¨‡•ç‡§∞‡§æ‡§π‡•ç‡§Æ‡§£ ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§®‡§π‡•Ä‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø ‡§¶‡•á‡§µ‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§æ‡§∏‡•ç‡§§‡•Å‡§ï‡§æ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§ï‡§∞‡•ç‡§Æ‡§æ ‡§•‡•Ä .',\n",
       "  '‡§Ö‡§®‡•á‡§ï ‡§∏‡§Æ‡•Å‡§¶‡§æ‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§π ‡§ï‡•ã ‡§®‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§µ‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ ‡§π‡•à‡§Ç , ‡§§‡§æ‡§ï‡§ø ‡§™‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§ú‡•Ä‡§µ‡•ã‡§Ç ‡§ï‡•ã ‡§Ü‡§π‡§æ‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã ‡§∏‡§ï‡•á .'],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "254148e3-c4d2-442e-a336-877747037695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3566580.65it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3557027.90it/s]\n",
      "Generating Labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [01:18<00:00, 33264.76it/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='682' max='782328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   682/782328 00:38 < 12:19:38, 17.61 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    152\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    153\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/amp/autocast_mode.py:12\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1862\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1862\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1876\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:553\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=3000000):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len=64):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence and align labels with tokens\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Get input ids and attention mask\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Align the labels with tokens\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Map word_ids to tokens\n",
    "\n",
    "        labels_aligned = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels_aligned.append(-100)  # Ignore these tokens (e.g., padding tokens)\n",
    "            else:\n",
    "                labels_aligned.append(label[word_id] if word_id < len(label) else 0)\n",
    "        \n",
    "        labels_aligned = torch.tensor(labels_aligned)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_aligned\n",
    "        }\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TokenClassificationDataset(sentences, labels, tokenizer)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Load pre-trained BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2  # Binary classification: correct or incorrect\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./token_classification_results\",\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",          # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = torch.argmax(torch.tensor(pred.predictions), axis=-1).flatten()\n",
    "\n",
    "    # Filter out -100 labels (ignored labels for padding, etc.)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./token_error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./token_error_detection_model\")\n",
    "\n",
    "# Make predictions for new sentence\n",
    "sentence = \"‡§Ø‡§π ‡§è‡§ï ‡§ó‡§≤‡§§ ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to CUDA if available\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "# Map predictions back to tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "for token, prediction in zip(tokens, predictions[0][:len(tokens)]):\n",
    "    status = \"Incorrect\" if prediction == 1 else \"Correct\"\n",
    "    print(f\"Token: {token}, Status: {status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
