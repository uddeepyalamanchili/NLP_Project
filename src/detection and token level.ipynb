{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f86bba-6de8-449a-a437-c80f283f8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Load a smaller subset of sentences and labels for initial testing\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=5):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a68907-d675-4141-ac3b-cf5196342e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3319468.67it/s]\n",
      "Reading Correct Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3472020.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा थी .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा हैं , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .', 'डीएनए क्षति और उत्परिवर्तन के बीच अंतर करना अत्यंत महत्वपूर्ण हैं .', 'यह खाना बनाने के काम आती है .', 'फ़िल्म का एल्बम अधिकार ज़ी म्यूजिक कंपनी द्वारा अधिगृहीत किए गए थे , और एल्बम को ११ मार्च २०१७ को रिलीज़ किया गया था .', 'तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा था .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा है , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .', 'डीएनए क्षति और उत्परिवर्तन के बीच अंतर करना अत्यंत महत्वपूर्ण है .', 'यह खाना बनाने के काम आता है .', 'फ़िल्म के एल्बम अधिकार ज़ी म्यूजिक कंपनी द्वारा अधिगृहीत किए गए थे , और एल्बम को ११ मार्च २०१७ को रिलीज़ किया गया था .'] [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Tokenizing Sentences: 100%|██████████| 1/1 [00:00<00:00, 29.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "print(len(sentences), sentences, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize sentences in smaller batches to avoid memory overload\n",
    "batch_size = 500\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(sentences), batch_size), desc=\"Batch Tokenizing Sentences\"):\n",
    "    batch_sentences = sentences[i:i+batch_size]\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch_sentences,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,  # Reduced max length for faster processing\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids_list.append(tokenized_batch[\"input_ids\"])\n",
    "    attention_mask_list.append(tokenized_batch[\"attention_mask\"])\n",
    "\n",
    "# Concatenate tokenized tensors\n",
    "tokenized_inputs = {\n",
    "    \"input_ids\": torch.cat(input_ids_list, dim=0),\n",
    "    \"attention_mask\": torch.cat(attention_mask_list, dim=0)\n",
    "}\n",
    "del input_ids_list, attention_mask_list  # Free up memory after concatenation\n",
    "gc.collect()  # Explicit garbage collection\n",
    "\n",
    "# Convert labels to a tensor\n",
    "labels = torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d514eb-11e4-4cfb-920e-5a3fb4854293",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Dataset with Pre-tokenized Inputs\n",
    "class PreTokenizedSentenceDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.tokenized_inputs.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PreTokenizedSentenceDataset(tokenized_inputs, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb59715c-7181-47e0-b86e-bf3b141a78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/3 00:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706055</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=0.1769561767578125, metrics={'train_runtime': 7.7626, 'train_samples_per_second': 3.092, 'train_steps_per_second': 0.386, 'total_flos': 264934797312.0, 'train_loss': 0.1769561767578125, 'epoch': 2.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification and move it to the GPU if available\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Define Training Arguments with Improvements\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# Define the Trainer with compute_metrics and early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "\n",
    "# Train the Model with a Progress Bar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e4e553-003b-4f9e-866b-15afcb290a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7060546875, 'eval_accuracy': 0.5, 'eval_runtime': 0.0083, 'eval_samples_per_second': 241.747, 'eval_steps_per_second': 120.873, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./error_detection_model/tokenizer_config.json',\n",
       " './error_detection_model/special_tokens_map.json',\n",
       " './error_detection_model/vocab.txt',\n",
       " './error_detection_model/added_tokens.json',\n",
       " './error_detection_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the Model\n",
    "model.save_pretrained(\"./error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./error_detection_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89d7000-ac2d-4e19-b180-3eff44b2d1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is error-free.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "sentence = \"उसकी प्रतिभा की गहराई किसी अनजाने समुद्र जैसा है\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get prediction\n",
    "prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "if prediction == 1:\n",
    "    print(\"Sentence contains errors.\")\n",
    "else:\n",
    "    print(\"Sentence is error-free.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed6e0984-b6df-41bd-98a2-3327b5f3ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels2(src_path, tgt_path, limit=20000000):  # Limit to 20,000 for reduced training time\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    sentences = incorrect_sentences[:limit] + correct_sentences[:limit]\n",
    "    labels = [1] * min(len(incorrect_sentences), limit) + [0] * min(len(correct_sentences), limit)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d8a09ff-8a7c-49cc-8178-60b6185f9fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3441489.78it/s]\n",
      "Reading Correct Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3369125.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels2(src_path, tgt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f23112-beda-4cb7-b8fc-0c8e8765e0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['मधु ने एक महल बनाया और उस स्थान की नाम मधुपुरी ( संभवतः मथुरा अब ) रखा .',\n",
       "  'किसी एक उल्लिखित क्षेत्र में पाई जाने वाली भाषा संबंधी विशेषताओं का व्यवस्थित अध्ययन भाषा भूगोल या बोली भूगोल की अंतर्गत आता है .',\n",
       "  '\" अक्सर पूछे जाने वाली प्रश्न \" लॉन्च किया , .',\n",
       "  'यह जरूरी नहीं है कि सहयोग के लिये नेतृत्व का आवश्यक होता है .',\n",
       "  'स्कूल के पूर्व छात्रों को आर्केशियन कहा जाता हैं .',\n",
       "  'अभिनेता सनी देओल को विशेष ज्युरी अवार्ड के तौर पर राष्ट्रीय फिल्म पुरस्कार की ओर से सर्वश्रेष्ठ अभिनेता का खिताब नवाजी गया .',\n",
       "  'महाभारत के युद्ध में काशिराज ने पांडवों के साथ दिया था .',\n",
       "  '१९०१ में कोटद्वार को नगर का दर्जा दिया गया था , तथा उसी वर्ष हुई प्रथम जनगणना में नगर का जनसंख्या १०२९ थी .'],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "numbers = random.sample(range(2607707, 2607857), k=20)\n",
    "newsentences = []\n",
    "newlabels = []\n",
    "for i in numbers:\n",
    "    if labels[i]!=0:\n",
    "        newsentences.append(sentences[i])\n",
    "        newlabels.append(labels[i])\n",
    "\n",
    "newsentences, newlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c00461-a98f-4bdc-86df-357660929a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence contains errors.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n",
      "Sentence is error-free.\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with the Trained Model\n",
    "# Load the trained model and tokenizer for inference\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./error_detection_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./error_detection_model\")\n",
    "\n",
    "# Predict for a new sentence\n",
    "for i in newsentences:\n",
    "    sentence =i\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Disable gradient calculations for faster inference\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    if prediction == 1:\n",
    "        print(\"Sentence contains errors.\")\n",
    "    else:\n",
    "        print(\"Sentence is error-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19574aae-0686-43f3-8419-0af5d2636f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_and_labels3(src_file, tgt_file, limit = 10):\n",
    "    with open(src_file,'r', encoding='utf-8') as src, open(tgt_file,'r', encoding = 'utf-8') as tgt:\n",
    "        IncorrectSentences = [line.strip() for line in src.readlines()]\n",
    "        CorrectSentences = [line.strip() for line in tgt.readlines()]\n",
    "        print(IncorrectSentences[:limit])\n",
    "        print(CorrectSentences[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ca6c291-f158-4add-be68-260cc362409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा थी .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा हैं , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .', 'डीएनए क्षति और उत्परिवर्तन के बीच अंतर करना अत्यंत महत्वपूर्ण हैं .', 'यह खाना बनाने के काम आती है .', 'फ़िल्म का एल्बम अधिकार ज़ी म्यूजिक कंपनी द्वारा अधिगृहीत किए गए थे , और एल्बम को ११ मार्च २०१७ को रिलीज़ किया गया था .', 'यहां पहाड़ों के मध्य फैली झील के आसपास किसी गेस्ट हाउस में रुक प्रकृति का मजा उठा सकतीं हैं .', 'रामनाथ उसकी ईमानदारी देख कर खुश हो जाता है और अपने कार्यालय में उसे क्लर्क की नौकरी दे देतीं है .', 'कई दिनों के बाद एक सत्रह वर्षीय केबिन कर्मचारी भूख और समुद्री पानी पी लेनी की वजह से बेहोश हो जाता है .', 'इनकी रहन सहन बहुत ही साधारण था .', 'हैदराबाद , मई 2007 हैदराबाद का मक्का मस्जिद में विस्फोट में 11 की मौत .']\n",
      "['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा था .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा है , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .', 'डीएनए क्षति और उत्परिवर्तन के बीच अंतर करना अत्यंत महत्वपूर्ण है .', 'यह खाना बनाने के काम आता है .', 'फ़िल्म के एल्बम अधिकार ज़ी म्यूजिक कंपनी द्वारा अधिगृहीत किए गए थे , और एल्बम को ११ मार्च २०१७ को रिलीज़ किया गया था .', 'यहां पहाड़ों के मध्य फैली झील के आसपास किसी गेस्ट हाउस में रुक प्रकृति का मजा उठा सकते हैं .', 'रामनाथ उसकी ईमानदारी देख कर खुश हो जाता है और अपने कार्यालय में उसे क्लर्क की नौकरी दे देता है .', 'कई दिनों के बाद एक सत्रह वर्षीय केबिन कर्मचारी भूख और समुद्री पानी पी लेने की वजह से बेहोश हो जाता है .', 'इनका रहन सहन बहुत ही साधारण था .', 'हैदराबाद , मई 2007 हैदराबाद की मक्का मस्जिद में विस्फोट में 11 की मौत .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1f4b0d-74a1-4654-9813-96008550bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3396297.64it/s]\n",
      "Reading Correct Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3524000.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा था .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा है , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .']\n",
      "['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा थी .', 'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा हैं , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Labels: 100%|██████████| 2/2 [00:00<00:00, 9157.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels3(src_path, tgt_path, limit=2):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "    print(correct_sentences)\n",
    "    print(incorrect_sentences)\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels3(src_path, tgt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87e2e2d-a69a-4754-9aa6-d89b6d9b7477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['तब राजा को आभास हुआ कि ब्राह्मण और कोई नहीं बल्कि देवों का वास्तुकार विश्वकर्मा थी .',\n",
       "  'अनेक समुदायों में देह को नदी में प्रवाहित करने की परंपरा हैं , ताकि पानी में रहने वाले विभिन्न जीवों को आहार उपलब्ध हो सके .'],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254148e3-c4d2-442e-a336-877747037695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3390690.16it/s]\n",
      "Reading Correct Sentences: 100%|██████████| 2607757/2607757 [00:00<00:00, 3512584.87it/s]\n",
      "Generating Labels: 100%|██████████| 2607757/2607757 [01:20<00:00, 32475.84it/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3859' max='782328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3859/782328 07:39 < 25:44:57, 8.40 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    152\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    153\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2427\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2425\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2426\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2427\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2429\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:5045\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5045\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5046\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5047\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    563\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:184\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 184\u001b[0m         {\n\u001b[1;32m    185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking, skip_keys\u001b[38;5;241m=\u001b[39mskip_keys)\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:185\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         skip_keys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[1;32m    184\u001b[0m         {\n\u001b[0;32m--> 185\u001b[0m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    187\u001b[0m         }\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    154\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example function to generate token-level labels based on incorrect and correct sentence pairs\n",
    "def generate_token_labels(incorrect_sentence, correct_sentence):\n",
    "    # Tokenize sentences\n",
    "    incorrect_tokens = incorrect_sentence.split()\n",
    "    correct_tokens = correct_sentence.split()\n",
    "    \n",
    "    # Use SequenceMatcher to find matching and differing blocks\n",
    "    matcher = SequenceMatcher(None, incorrect_tokens, correct_tokens)\n",
    "    labels = [0] * len(incorrect_tokens)  # Initialize all labels as correct (0)\n",
    "\n",
    "    # Mark differing tokens as incorrect\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag in (\"replace\", \"delete\"):\n",
    "            for i in range(i1, i2):\n",
    "                labels[i] = 1  # Mark as incorrect\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Read sentences and generate token-level labels\n",
    "def read_sentences_and_labels(src_path, tgt_path, limit=3000000):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "\n",
    "    # Limit dataset size for faster processing\n",
    "    incorrect_sentences = incorrect_sentences[:limit]\n",
    "    correct_sentences = correct_sentences[:limit]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # Generate token-level labels for each sentence pair\n",
    "    for incorrect, correct in tqdm(zip(incorrect_sentences, correct_sentences), desc=\"Generating Labels\", total=len(incorrect_sentences)):\n",
    "        sentences.append(incorrect)\n",
    "        token_labels = generate_token_labels(incorrect, correct)\n",
    "        labels.append(token_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load sentences and labels (limited subset)\n",
    "src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "sentences, labels = read_sentences_and_labels(src_path, tgt_path)\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len=64):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence and align labels with tokens\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize with padding and truncation\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Get input ids and attention mask\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        # Align the labels with tokens\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Map word_ids to tokens\n",
    "\n",
    "        labels_aligned = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels_aligned.append(-100)  # Ignore these tokens (e.g., padding tokens)\n",
    "            else:\n",
    "                labels_aligned.append(label[word_id] if word_id < len(label) else 0)\n",
    "        \n",
    "        labels_aligned = torch.tensor(labels_aligned)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_aligned\n",
    "        }\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TokenClassificationDataset(sentences, labels, tokenizer)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Load pre-trained BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2  # Binary classification: correct or incorrect\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./token_classification_results\",\n",
    "    evaluation_strategy=\"epoch\",    # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",          # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = torch.argmax(torch.tensor(pred.predictions), axis=-1).flatten()\n",
    "\n",
    "    # Filter out -100 labels (ignored labels for padding, etc.)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./token_error_detection_model\")\n",
    "tokenizer.save_pretrained(\"./token_error_detection_model\")\n",
    "\n",
    "# Make predictions for new sentence\n",
    "sentence = \"यह एक गलत वाक्य है।\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to CUDA if available\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, axis=-1)\n",
    "\n",
    "# Map predictions back to tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "for token, prediction in zip(tokens, predictions[0][:len(tokens)]):\n",
    "    status = \"Incorrect\" if prediction == 1 else \"Correct\"\n",
    "    print(f\"Token: {token}, Status: {status}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
