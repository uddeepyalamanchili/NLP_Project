{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcbb3df-81c2-446a-97a4-199709ce5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3385905.95it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2607757/2607757 [00:00<00:00, 3517563.21it/s]\n",
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 521552/521552 [00:00<00:00, 3303649.17it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 521552/521552 [00:00<00:00, 3271656.98it/s]\n",
      "Reading Incorrect Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13187/13187 [00:00<00:00, 2960143.80it/s]\n",
      "Reading Correct Sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13187/13187 [00:00<00:00, 3077923.59it/s]\n",
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.083800</td>\n",
       "      <td>0.386179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.224713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.192019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.180972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.174762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nchukka/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17476209998130798, 'eval_runtime': 0.5792, 'eval_samples_per_second': 345.283, 'eval_steps_per_second': 43.16, 'epoch': 5.0}\n",
      "Evaluating on test dataset:\n",
      "{'eval_loss': 0.21930748224258423, 'eval_runtime': 0.5763, 'eval_samples_per_second': 347.06, 'eval_steps_per_second': 43.383, 'epoch': 5.0}\n",
      "Corrected Sentence: ‡§â‡§∏‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ ‡§ï‡§æ ‡§ó‡§π‡§∞‡§æ‡§à ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ú‡•à‡§∏‡•á ‡§π‡•à .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Free up memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load the tokenizer and model for MarianMT\n",
    "model_name = \"Helsinki-NLP/opus-mt-hi-en\"  # Using a Hindi to English model for demonstration purposes\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Load the datasets\n",
    "def read_sentences(src_path, tgt_path, limit=None):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as src_file, open(tgt_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        incorrect_sentences = [line.strip() for line in tqdm(src_file.readlines(), desc=\"Reading Incorrect Sentences\")]\n",
    "        correct_sentences = [line.strip() for line in tqdm(tgt_file.readlines(), desc=\"Reading Correct Sentences\")]\n",
    "    if limit:\n",
    "        incorrect_sentences = incorrect_sentences[:limit]\n",
    "        correct_sentences = correct_sentences[:limit]\n",
    "    return incorrect_sentences, correct_sentences\n",
    "\n",
    "# Paths for training, validation, and test datasets\n",
    "train_src_path = \"wikiExtractsData/data/train_merge.src\"\n",
    "train_tgt_path = \"wikiExtractsData/data/train_merge.tgt\"\n",
    "valid_src_path = \"wikiExtractsData/data/valid.src\"\n",
    "valid_tgt_path = \"wikiExtractsData/data/valid.tgt\"\n",
    "test_src_path = \"Wiki-edits/hiwiki.extracted.clean.src\"\n",
    "test_tgt_path = \"Wiki-edits/hiwiki.extracted.clean.trg\"\n",
    "\n",
    "# Load Training, Validation, and Test Data (Limited subset for faster training initially)\n",
    "train_incorrect, train_correct = read_sentences(train_src_path, train_tgt_path, limit=1000000)\n",
    "valid_incorrect, valid_correct = read_sentences(valid_src_path, valid_tgt_path, limit=200000)\n",
    "test_incorrect, test_correct = read_sentences(test_src_path, test_tgt_path, limit=2500)\n",
    "\n",
    "# Define Dataset class for sentence correction\n",
    "class SentenceCorrectionDataset(Dataset):\n",
    "    def __init__(self, incorrect_sentences, correct_sentences, tokenizer, max_len=128):\n",
    "        self.incorrect_sentences = incorrect_sentences\n",
    "        self.correct_sentences = correct_sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.incorrect_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        incorrect = self.incorrect_sentences[idx]\n",
    "        correct = self.correct_sentences[idx]\n",
    "\n",
    "        # Add a prefix to indicate the task type\n",
    "        input_text = incorrect\n",
    "        target_text = correct\n",
    "\n",
    "        # Tokenize input and target texts\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, \n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        targets = self.tokenizer(\n",
    "            target_text, \n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Squeeze tensors to remove unnecessary dimensions\n",
    "        input_ids = inputs.input_ids.squeeze()\n",
    "        attention_mask = inputs.attention_mask.squeeze()\n",
    "        labels = targets.input_ids.squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Create datasets using the small subset\n",
    "train_dataset = SentenceCorrectionDataset(train_incorrect, train_correct, tokenizer)\n",
    "valid_dataset = SentenceCorrectionDataset(valid_incorrect, valid_correct, tokenizer)\n",
    "test_dataset = SentenceCorrectionDataset(test_incorrect, test_correct, tokenizer)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,  # Standard learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,  # Adjust batch size to fit in GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,  # More epochs for better fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training if CUDA is available\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")\n",
    "\n",
    "# Train the Model using the subset\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the validation dataset\n",
    "print(\"Evaluating on validation dataset:\")\n",
    "validation_results = trainer.evaluate()\n",
    "print(validation_results)\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "print(\"Evaluating on test dataset:\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(test_results)\n",
    "\n",
    "# Save the Model\n",
    "model.save_pretrained(\"./sentence_correction_model\")\n",
    "tokenizer.save_pretrained(\"./sentence_correction_model\")\n",
    "\n",
    "# Make Predictions with the Trained Model\n",
    "model = MarianMTModel.from_pretrained(\"./sentence_correction_model\").to(device)\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"./sentence_correction_model\")\n",
    "\n",
    "# Predict for a new incorrect sentence\n",
    "incorrect_sentence = \"‡§â‡§∏‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§≠‡§æ ‡§ï‡•Ä ‡§ó‡§π‡§∞‡§æ‡§à ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§®‡§ú‡§æ‡§®‡•á ‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§ú‡•à‡§∏‡§æ ‡§π‡•à\"\n",
    "inputs = tokenizer(incorrect_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).to(device)\n",
    "\n",
    "# Generate corrected sentence\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids=inputs[\"input_ids\"], max_length=128, num_beams=5, early_stopping=True)\n",
    "\n",
    "# Decode the output to get the corrected sentence\n",
    "corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(f\"Corrected Sentence: {corrected_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
